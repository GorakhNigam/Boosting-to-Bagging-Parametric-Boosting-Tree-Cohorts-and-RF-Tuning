{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9bc7a54",
   "metadata": {},
   "source": [
    "In this example you will implement and evaluate bagging for a classification\n",
    "problem.\n",
    "1. Load the Breast Cancer dataset from the mlbench package.\n",
    "2. Split the data into training and testing sets.\n",
    "3. Implement bagging using the ipred package to create an ensemble of classification trees.\n",
    "4. Evaluate the performance of the bagged model using accuracy, precision, recall, and F1-\n",
    "score.\n",
    "5. Compare the performance of the bagged model with that of a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a15bf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Single Decision Tree (test) ===\n",
      "accuracy : 0.9580\n",
      "precision: 0.9796\n",
      "recall   : 0.9057\n",
      "f1       : 0.9412\n",
      "\n",
      "Classification report (Tree):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   benign(0)       0.95      0.99      0.97        90\n",
      "malignant(1)       0.98      0.91      0.94        53\n",
      "\n",
      "    accuracy                           0.96       143\n",
      "   macro avg       0.96      0.95      0.95       143\n",
      "weighted avg       0.96      0.96      0.96       143\n",
      "\n",
      "\n",
      "=== Bagged Trees (test) ===\n",
      "accuracy : 0.9650\n",
      "precision: 1.0000\n",
      "recall   : 0.9057\n",
      "f1       : 0.9505\n",
      "OOB accuracy (Bagging): 0.9554\n",
      "\n",
      "Classification report (Bagging):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   benign(0)       0.95      1.00      0.97        90\n",
      "malignant(1)       1.00      0.91      0.95        53\n",
      "\n",
      "    accuracy                           0.97       143\n",
      "   macro avg       0.97      0.95      0.96       143\n",
      "weighted avg       0.97      0.97      0.96       143\n",
      "\n",
      "\n",
      "=== Summary ===\n",
      "Model               Acc    Prec   Recall  F1\n",
      "Decision Tree       0.958  0.980  0.906  0.941\n",
      "Bagging (200 trees)  0.965  1.000  0.906  0.950\n"
     ]
    }
   ],
   "source": [
    "# Bagging vs Single Tree (Python equivalent of mlbench/ipred)\n",
    "# Dataset: scikit-learn's Wisconsin Diagnostic Breast Cancer (binary classification)\n",
    "# Metrics: accuracy, precision, recall, F1 (positive class = malignant)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Load data (like mlbench::BreastCancer; here we use sklearn's WDBC)\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "# Map to 1=malignant, 0=benign so positive class is malignant\n",
    "y = (data.target == 0).astype(int)  # in sklearn: 0=malignant, 1=benign\n",
    "\n",
    "# Train/test split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Models\n",
    "# Single CART-like tree (unpruned/high-variance is typical baseline for bagging)\n",
    "tree = DecisionTreeClassifier(\n",
    "    criterion=\"gini\",\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "tree.fit(X_tr, y_tr)\n",
    "\n",
    "# Bagging of trees (Python analog of ipred::bagging)\n",
    "bag = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(\n",
    "        criterion=\"gini\", max_depth=None, min_samples_leaf=1\n",
    "    ),\n",
    "    n_estimators=200,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,          # out-of-bag estimate like ipred\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "bag.fit(X_tr, y_tr)\n",
    "\n",
    "# Evaluate helpers\n",
    "def metrics(y_true, y_prob_or_pred):\n",
    "    # accept probability or hard labels\n",
    "    if y_prob_or_pred.ndim == 1 and set(np.unique(y_prob_or_pred)) <= {0,1}:\n",
    "        y_pred = y_prob_or_pred\n",
    "    else:\n",
    "        y_pred = (y_prob_or_pred >= 0.5).astype(int)\n",
    "    return dict(\n",
    "        accuracy = accuracy_score(y_true, y_pred),\n",
    "        precision= precision_score(y_true, y_pred, zero_division=0),\n",
    "        recall   = recall_score(y_true, y_pred, zero_division=0),\n",
    "        f1       = f1_score(y_true, y_pred, zero_division=0),\n",
    "    )\n",
    "\n",
    "# Predictions\n",
    "tree_pred = tree.predict(X_te)\n",
    "bag_pred  = bag.predict(X_te)\n",
    "\n",
    "tree_metrics = metrics(y_te, tree_pred)\n",
    "bag_metrics  = metrics(y_te, bag_pred)\n",
    "\n",
    "# Print comparison\n",
    "print(\"=== Single Decision Tree (test) ===\")\n",
    "for k,v in tree_metrics.items():\n",
    "    print(f\"{k:9s}: {v:.4f}\")\n",
    "print(\"\\nClassification report (Tree):\")\n",
    "print(classification_report(y_te, tree_pred, target_names=[\"benign(0)\",\"malignant(1)\"], zero_division=0))\n",
    "\n",
    "print(\"\\n=== Bagged Trees (test) ===\")\n",
    "for k,v in bag_metrics.items():\n",
    "    print(f\"{k:9s}: {v:.4f}\")\n",
    "print(f\"OOB accuracy (Bagging): {bag.oob_score_:.4f}\")\n",
    "print(\"\\nClassification report (Bagging):\")\n",
    "print(classification_report(y_te, bag_pred, target_names=[\"benign(0)\",\"malignant(1)\"], zero_division=0))\n",
    "\n",
    "# Optional: compact side-by-side summary\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"{'Model':18s}  Acc    Prec   Recall  F1\")\n",
    "print(f\"{'Decision Tree':18s}  {tree_metrics['accuracy']:.3f}  {tree_metrics['precision']:.3f}  {tree_metrics['recall']:.3f}  {tree_metrics['f1']:.3f}\")\n",
    "print(f\"{'Bagging (200 trees)':18s}  {bag_metrics['accuracy']:.3f}  {bag_metrics['precision']:.3f}  {bag_metrics['recall']:.3f}  {bag_metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e47eb",
   "metadata": {},
   "source": [
    "Observation/comparision\n",
    "1. Accuracy: 0.958 to 0.965 (increased)\n",
    "2. Precision (malignant as positive): 0.980 to 1.000 (increased — no false positives)\n",
    "3. Recall: 0.906 to 0.906 (almost same)\n",
    "4. F1: 0.941 to 0.951 (increased)\n",
    "5. Implied confusion matrices (test n=143; 90 benign, 53 malignant)\n",
    "6. Tree: TP≈48, FN≈5, FP≈1, TN≈89 and accuracy (48+89)/143≈0.958.\n",
    "7. Bagging: TP≈48, FN≈5, FP=0, TN=90 and accuracy (48+90)/143≈0.965.\n",
    "•\tInterpretation\n",
    "\t•\tBagging averages many unstable trees hence variance reduction.\n",
    "\t\tHere it eliminated the single false positive (hence precision=1.0) while the same hard malignant cases remained misclassified by most trees (recall unchanged).\n",
    "8. OOB accuracy 0.955 is close to test 0.965 hence out-of-bag gives a trustworthy generalization estimate.\n",
    "•\tWhat to do next (if recall matters more)\n",
    "\t•\tUse probability averaging from bagging and lower the decision threshold (<0.5) to trade a little precision for higher recall.\n",
    "\t•\tAdd class weights (class_weight='balanced') or stratified bootstraps to target malignant sensitivity.\n",
    "\t•\tConsider Random Forests (bagging + feature subsampling) to further decorrelate trees, or boosting to reduce bias on hard cases.\n",
    "\n",
    "Takeaway: Bagging improved overall performance by removing spurious malignant calls (higher precision and F1) with essentially the same sensitivity; classic variance-reduction behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252296e",
   "metadata": {},
   "source": [
    "Construct a regression simulation that will be used for studying the effect of\n",
    "nodesize on RF performance.\n",
    "1. Simulate the data and run a RF regression analysis under different values of nodesize.\n",
    "Determine the optimal nodesize value by minimizing the estimated generalization error of\n",
    "the OOB ensemble.\n",
    "2. Using simulated data sets repeat part 1 but use test set estimated generalization error to\n",
    "obtain the optimal nodesize.\n",
    "3. Compare your results of parts 2 and 3 and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeecad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== One split (Part 1 vs Part 2) ===\n",
      "   nodesize   oob_mse  test_mse\n",
      "0         1  3.591798  3.334139\n",
      "1         2  3.663401  3.400498\n",
      "2         5  4.017821  3.731814\n",
      "3        10  4.545869  4.259320\n",
      "4        20  5.358853  5.062020\n",
      "5        30  6.039767  5.739885\n",
      "6        50  7.038834  6.720530\n",
      "7        80  8.320833  7.994824\n",
      "8       120  9.715746  9.377452\n",
      "\n",
      "OOB-picked nodesize: 1 | Test MSE at that pick: 3.3341385116016804\n",
      "Test-picked nodesize: 1 | Test MSE at that pick: 3.3341385116016804\n",
      "\n",
      "=== Selection frequencies over seeds ===\n",
      "OOB-selected nodesize:\n",
      " oob_best_nodesize\n",
      "1    20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test-selected nodesize:\n",
      " test_best_nodesize\n",
      "1    20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Average test MSE (lower is better) ===\n",
      "When selecting by OOB:  3.397484452667908\n",
      "When selecting by Test: 3.397484452667908\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Simulation (Friedman-1)\n",
    "\n",
    "def simulate_friedman1(n=3000, p=10, sigma=1.0, rng=None):\n",
    "    \"\"\"\n",
    "    Returns X (n x p), y with Friedman-1 signal on first 5 dims + Gaussian noise.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(rng)\n",
    "    X = rng.uniform(0.0, 1.0, size=(n, p))\n",
    "    y = (10*np.sin(np.pi*X[:,0]*X[:,1]) +\n",
    "         20*(X[:,2]-0.5)**2 +\n",
    "         10*X[:,3] +\n",
    "         5*X[:,4] +\n",
    "         rng.normal(0.0, sigma, size=n))\n",
    "    return X, y\n",
    "\n",
    "# One-split evaluation\n",
    "def sweep_nodesize_once(\n",
    "    n=3000, p=10, sigma=1.0, test_size=0.3, seed=0,\n",
    "    nodesize_grid=(1, 2, 5, 10, 20, 30, 50),\n",
    "    rf_kwargs=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulate, split, fit a RandomForest for each nodesize.\n",
    "    Returns a DataFrame with OOB and test metrics and the chosen nodesize by each criterion.\n",
    "    \"\"\"\n",
    "    rf_kwargs = rf_kwargs or {}\n",
    "    X, y = simulate_friedman1(n=n, p=p, sigma=sigma, rng=seed)\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    rows = []\n",
    "    for nodesize in nodesize_grid:\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=500,\n",
    "            bootstrap=True,\n",
    "            oob_score=True,            # enable OOB predictions\n",
    "            min_samples_leaf=nodesize, # <-- \"nodesize\"\n",
    "            max_features=\"sqrt\",\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            **rf_kwargs\n",
    "        )\n",
    "        rf.fit(X_tr, y_tr)\n",
    "\n",
    "        # OOB MSE from OOB predictions\n",
    "        oob_pred = rf.oob_prediction_\n",
    "        # oob_prediction_ has length len(X_tr); compute its MSE vs y_tr\n",
    "        oob_mse = float(np.mean((y_tr - oob_pred)**2))\n",
    "\n",
    "        # Test MSE\n",
    "        te_pred = rf.predict(X_te)\n",
    "        te_mse = float(mean_squared_error(y_te, te_pred))\n",
    "\n",
    "        rows.append({\n",
    "            \"nodesize\": nodesize,\n",
    "            \"oob_mse\": oob_mse,\n",
    "            \"test_mse\": te_mse\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"nodesize\").reset_index(drop=True)\n",
    "    # winners by criterion\n",
    "    idx_oob  = int(df[\"oob_mse\"].idxmin())\n",
    "    idx_test = int(df[\"test_mse\"].idxmin())\n",
    "    out = {\n",
    "        \"table\": df,\n",
    "        \"oob_best_nodesize\": int(df.loc[idx_oob, \"nodesize\"]),\n",
    "        \"oob_best_oob_mse\": float(df.loc[idx_oob, \"oob_mse\"]),\n",
    "        \"oob_best_test_mse\": float(df.loc[idx_oob, \"test_mse\"]),   # generalization if selected by OOB\n",
    "        \"test_best_nodesize\": int(df.loc[idx_test, \"nodesize\"]),\n",
    "        \"test_best_test_mse\": float(df.loc[idx_test, \"test_mse\"]),\n",
    "        \"seed\": seed\n",
    "    }\n",
    "    return out\n",
    "\n",
    "# 3) Replicated experiment\n",
    "\n",
    "def run_experiment(\n",
    "    reps=20, n=3000, p=10, sigma=1.0, nodesize_grid=(1,2,5,10,20,30,50), test_size=0.3\n",
    "):\n",
    "    summary_rows = []\n",
    "    per_seed_tables = []\n",
    "\n",
    "    for s in range(reps):\n",
    "        res = sweep_nodesize_once(\n",
    "            n=n, p=p, sigma=sigma, seed=101 + s,\n",
    "            nodesize_grid=nodesize_grid, test_size=test_size\n",
    "        )\n",
    "        per_seed_tables.append(res[\"table\"].assign(seed=res[\"seed\"]))\n",
    "        summary_rows.append({\n",
    "            \"seed\": res[\"seed\"],\n",
    "            \"oob_best_nodesize\": res[\"oob_best_nodesize\"],\n",
    "            \"oob_best_test_mse\": res[\"oob_best_test_mse\"],\n",
    "            \"test_best_nodesize\": res[\"test_best_nodesize\"],\n",
    "            \"test_best_test_mse\": res[\"test_best_test_mse\"],\n",
    "        })\n",
    "\n",
    "    perf = pd.DataFrame(summary_rows)\n",
    "    counts_oob  = perf[\"oob_best_nodesize\"].value_counts().sort_index()\n",
    "    counts_test = perf[\"test_best_nodesize\"].value_counts().sort_index()\n",
    "\n",
    "    print(\"\\n=== Selection frequencies over seeds ===\")\n",
    "    print(\"OOB-selected nodesize:\\n\", counts_oob)\n",
    "    print(\"\\nTest-selected nodesize:\\n\", counts_test)\n",
    "\n",
    "    print(\"\\n=== Average test MSE (lower is better) ===\")\n",
    "    print(\"When selecting by OOB: \", perf[\"oob_best_test_mse\"].mean())\n",
    "    print(\"When selecting by Test:\", perf[\"test_best_test_mse\"].mean())\n",
    "\n",
    "    # Return for further analysis/plotting if desired\n",
    "    return perf, pd.concat(per_seed_tables, ignore_index=True)\n",
    "\n",
    "# Example run\n",
    "if __name__ == \"__main__\":\n",
    "    # Grid is wide enough to show variance-bias tradeoff\n",
    "    GRID = (1, 2, 5, 10, 20, 30, 50, 80, 120)\n",
    "\n",
    "    # Single run (Part 1 + Part 2 on one split)\n",
    "    single = sweep_nodesize_once(nodesize_grid=GRID, seed=123, sigma=1.0)\n",
    "    print(\"\\n=== One split (Part 1 vs Part 2) ===\")\n",
    "    print(single[\"table\"])\n",
    "    print(\"\\nOOB-picked nodesize:\", single[\"oob_best_nodesize\"],\n",
    "          \"| Test MSE at that pick:\", single[\"oob_best_test_mse\"])\n",
    "    print(\"Test-picked nodesize:\", single[\"test_best_nodesize\"],\n",
    "          \"| Test MSE at that pick:\", single[\"test_best_test_mse\"])\n",
    "\n",
    "    # Replicates (Part 3)\n",
    "    perf, tables = run_experiment(reps=20, nodesize_grid=GRID, sigma=1.0)\n",
    "    # Optional: save tables to CSV for plotting externally\n",
    "    # tables.to_csv(\"rf_nodesize_sweep.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b8349",
   "metadata": {},
   "source": [
    "Observation\n",
    "1. Both OOB and test pick nodesize = 1 (deepest trees).\n",
    "2. MSE increases monotonically as nodesize grows so larger leaves = more bias.\n",
    "3. OOB MSE is consistently a bit higher than test MSE (expected: OOB uses ~63% unique samples per tree and less data per prediction).\n",
    "4. Your sim (Friedman-1, n=3000, sigma=1, 500 trees, bootstrap, max_features=√p) is variance-controlled which is aggregation of bagging, many trees, plenty of data tame the variance of deep trees.\n",
    "5. With variance already small, pushing nodesize up only raises bias, so test error worsens.\n",
    "6. When the optimum shifts upward\n",
    "7. Higher noise (sigma high), smaller n, fewer trees, or more aggressive feature subsampling can make deep trees too wiggly, then the best nodesize typically moves to 5–30.\n",
    "8. We'll also see a clearer U-shape in MSE vs nodesize under those settings.\n",
    "9. In our current high-signal, large-n setting, RF benefits from maximally deep trees; OOB selection matches test selection, so OOB is a good proxy for nodesize tuning here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
